{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1dc2c3fe-9299-42dc-aa08-12714b2f1212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Étiquette  Nombre_de_récurrence\n",
      "0                                      0                 67495\n",
      "1                      electric vehicles                  2675\n",
      "2                        electrification                  1253\n",
      "3                       electric vehicle                   558\n",
      "4              battery electric vehicles                   437\n",
      "...                                  ...                   ...\n",
      "23195          trolleybus infrastructure                     1\n",
      "23196  electrified public transportation                     1\n",
      "23197                       static model                     1\n",
      "23198                   computation time                     1\n",
      "23199       bidirectional direct current                     1\n",
      "\n",
      "[23200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#2. Relier les bulles connectées entre elles \n",
    "\n",
    "import dash\n",
    "from dash import html, dcc, Input, Output\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "### Etude du fichier csv et compte des étiquettes \n",
    "\n",
    "df = pd.read_csv(\"Data_20250506.csv\", sep=';')\n",
    "\n",
    "etiquettes_data = df.iloc[:, 11:]  # à partir de la colonne 12, on prend tous les mots clés des études scientifiques \n",
    "etiquettes_series = pd.Series(etiquettes_data.values.ravel())\n",
    "\n",
    "# Nettoyage des valeurs : on enlève les cases vides, les majuscules...car sinon on compterait différemment Fer et fer par exemple\n",
    "\n",
    "etiquettes_series = etiquettes_series.dropna().astype(str).str.strip().str.lower() \n",
    "\n",
    "etiquettes_comptes = etiquettes_series.value_counts().reset_index()  # compte le nombre d'occurences dans toute la littérature scientifique\n",
    "etiquettes_comptes.columns = ['Étiquette', 'Nombre_de_récurrence']\n",
    "\n",
    "print(etiquettes_comptes)\n",
    "\n",
    "\n",
    "\n",
    "#### A ce point la, certains mots sont encore compté deux fois différement  = electric vehicles et elecric vehicle par exmple !\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9acfb48c-95af-4a3a-9277-5c81338445ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ewenm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ewenm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenise \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "78a48ac7-9588-4dfc-b1c8-d609f64bcdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electric vehicles have been developed with the aims of saving energy and reducing carbon dioxide emissions. Furthermore, as a single bus can transport many people, buses produce low carbon dioxide emissions per person in comparison with cars. In recent years, community buses have been introduced to ensure the mobility of senior citizens. Therefore, a low-floor vehicle was adopted as the base vehicle in our latest project. The bus was demonstrated around the center of the city. It followed a circular route of distance 7.2 kilometers, the number of bus stops on the route was 27 and the driving time was 40 minutes per circuit. This demonstration was repeated three to four times a day for 14 days. During the 86 circuits of the demonstration, a total of 1,110 passengers used the bus and the total running distance was 776 kilometers. From these results, we concluded that the quantity of carbon dioxide emissions could be reduced by 37% using our proposed bus in comparison with that of a conventional diesel-engine bus.\n"
     ]
    }
   ],
   "source": [
    "text=df['AB_AI'][0]\n",
    "print(text)\n",
    "\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0bb61601-2438-4595-974e-9912bd5d481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de mots de text : 1026\n",
      "nombre de mots de words : 187\n",
      "nombre de mots de W ( sans ponctuation) : 88\n",
      "nombre de mots de W_sans_ED : 78\n",
      "['dioxide', 'four', 'quantity', 'stops', 'Furthermore', 'many', 'buses', '86', 'energy', 'passengers', 'time', 'number', 'cars', 'could', '1,110', 'low-floor', 'During', 'route', 'days', 'latest', 'around', 'senior', '776', 'minutes', 'project', 'This', '40', 'reducing', 'city', 'diesel-engine', 'The', 'total', 'produce', 'distance', 'per', 'citizens', 'emissions', 'aims', 'comparison', 'years', 'results', 'circular', 'circuit', 'community', 'times', '7.2', 'mobility', 'ensure', 'carbon', 'three', 'running', 'transport', 'people', '27', 'kilometers', 'It', 'bus', 'conventional', 'driving', 'vehicles', 'using', 'saving', 'single', 'demonstration', 'day', 'circuits', 'recent', '14', 'Electric', 'person', 'vehicle', 'base', 'center', 'In', '37', 'low', 'Therefore', 'From']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ewenm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Virons les stop words \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print('nombre de mots de text :' , len(text))\n",
    "words = word_tokenize(text)\n",
    "words_filtered = []\n",
    "print('nombre de mots de words :' , len(words))\n",
    "# on va renvoyer une liste des mots en ne gardant \"que les plus importants\". On enlève par exemple les \"is\", \"of... En somme, on ne garde que les mots clés.\n",
    "for w in words:\n",
    "  if w not in stopwords:\n",
    "    words_filtered.append(w)\n",
    "\n",
    "\n",
    "\n",
    "# J'enlève maintenant toute la ponctuation de ma liste de mots. \n",
    "ponctuation = ['.',',',';','?',':','!','%']\n",
    "words_sans_ponctuation=[x for x in words_filtered if x not in ponctuation]\n",
    "\n",
    "\n",
    "# on a beaucoup de doublons dans la liste : on les retire pour diminuer le nombre de mots \n",
    "W=list(set(words_sans_ponctuation))\n",
    "print('nombre de mots de W ( sans ponctuation) :' , len(W))\n",
    "# on fait un choix : on enlève les mots qui terminent par \"ED\" en anglais car on estime que ce sont des verbes, et non les principaux mots-clés. \n",
    "\n",
    "\n",
    "W_sans_ED = [w for w in W if not w.lower().endswith(\"ed\")] \n",
    "print('nombre de mots de W_sans_ED :' , len(W_sans_ED))\n",
    "print(W_sans_ED)\n",
    "\n",
    "# A ce stade on a réussi à réduire le nombre de mots clés de 1026 mots à 79. Ca rste encore beaucoup trop si on veut traiter beaucoup d'articles en meme temps. \n",
    "sans_maj = [w.lower() for w in W_sans_ED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "74bca924-39b4-4fb3-bddb-faafcd98466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de mots de mots_filtrés : 72\n",
      "['dioxide', 'four', 'quantity', 'stops', 'furthermore', 'many', 'buses', '86', 'energy', 'passengers', 'time', 'number', 'cars', 'could', '1,110', 'low-floor', 'route', 'days', 'latest', 'around', 'senior', '776', 'minutes', 'project', '40', 'reducing', 'city', 'diesel-engine', 'total', 'produce', 'distance', 'per', 'citizens', 'emissions', 'aims', 'comparison', 'years', 'results', 'circular', 'circuit', 'community', 'times', '7.2', 'mobility', 'ensure', 'carbon', 'three', 'running', 'transport', 'people', '27', 'kilometers', 'bus', 'conventional', 'driving', 'vehicles', 'using', 'saving', 'single', 'demonstration', 'day', 'circuits', 'recent', '14', 'electric', 'person', 'vehicle', 'base', 'center', '37', 'low', 'therefore']\n"
     ]
    }
   ],
   "source": [
    "# on enlève les majuscules pour pouvoir enlever les stopwords qui contenaient des majuscules \n",
    "mots_filtrés =[]\n",
    "for w in sans_maj:\n",
    "  if w not in stopwords:\n",
    "    mots_filtrés.append(w)\n",
    "\n",
    "print('nombre de mots de mots_filtrés :' , len(mots_filtrés))\n",
    "print(mots_filtrés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b68aa35b-c6f4-4045-a204-48c4a5db61a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ewenm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ewenm\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de mots de W_singuliers 67\n",
      "['furthermore', 'dioxide', 'driving', 'four', 'per', 'quantity', 'produce', 'result', 'electric', 'around', 'emission', 'comparison', 'year', 'using', 'saving', 'passenger', 'senior', 'kilometer', 'many', 'citizen', '86', 'energy', 'total', 'single', 'demonstration', 'circular', 'circuit', 'therefore', '776', 'time', 'day', 'community', '7.2', 'number', 'mobility', 'could', 'ensure', 'carbon', 'project', 'aim', '1,110', 'three', 'running', 'recent', '14', 'person', 'minute', 'vehicle', 'base', 'low-floor', '40', 'transport', 'reducing', 'people', 'center', 'route', '37', '27', 'stop', 'car', 'city', 'bus', 'diesel-engine', 'low', 'conventional', 'latest', 'distance']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Mise au singulier\n",
    "W_singuliers = list(set([lemmatizer.lemmatize(w, pos='n') for w in mots_filtrés]))\n",
    "\n",
    "print('nombre de mots de W_singuliers', len(W_singuliers))\n",
    "print(W_singuliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4f932361-a023-4f5f-a293-ebffa255639b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de mots de mots_assez_long 62\n",
      "['furthermore', 'dioxide', 'driving', 'four', 'per', 'quantity', 'produce', 'result', 'electric', 'around', 'emission', 'comparison', 'year', 'using', 'saving', 'passenger', 'senior', 'kilometer', 'many', 'citizen', 'energy', 'total', 'single', 'demonstration', 'circular', 'circuit', 'therefore', '776', 'time', 'day', 'community', '7.2', 'number', 'mobility', 'could', 'ensure', 'carbon', 'project', 'aim', '1,110', 'three', 'running', 'recent', 'person', 'minute', 'vehicle', 'base', 'low-floor', 'transport', 'reducing', 'people', 'center', 'route', 'stop', 'car', 'city', 'bus', 'diesel-engine', 'low', 'conventional', 'latest', 'distance']\n"
     ]
    }
   ],
   "source": [
    "# j'enlève les mots de moins de 3 caractères ; \n",
    "mots_assez_long = []\n",
    "for w in W_singuliers : \n",
    "    if len(w)>=3 : \n",
    "        mots_assez_long.append(w)\n",
    "\n",
    "print('nombre de mots de mots_assez_long', len(mots_assez_long))\n",
    "print(mots_assez_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f304b5e3-3d1a-4f2a-a5bd-c600c9655ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de mots de W_sans_nombres 59\n",
      "['furthermore', 'dioxide', 'driving', 'four', 'per', 'quantity', 'produce', 'result', 'electric', 'around', 'emission', 'comparison', 'year', 'using', 'saving', 'passenger', 'senior', 'kilometer', 'many', 'citizen', 'energy', 'total', 'single', 'demonstration', 'circular', 'circuit', 'therefore', 'time', 'day', 'community', 'number', 'mobility', 'could', 'ensure', 'carbon', 'project', 'aim', 'three', 'running', 'recent', 'person', 'minute', 'vehicle', 'base', 'low-floor', 'transport', 'reducing', 'people', 'center', 'route', 'stop', 'car', 'city', 'bus', 'diesel-engine', 'low', 'conventional', 'latest', 'distance']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def est_nombre(w):\n",
    "    return re.fullmatch(r\"[0-9]+([.,][0-9]+)?\", w) is not None\n",
    "\n",
    "W_sans_nombres = [w for w in mots_assez_long if not est_nombre(w)]\n",
    "\n",
    "print('nombre de mots de W_sans_nombres', len(W_sans_nombres))\n",
    "print(W_sans_nombres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2aa8e0-007a-4827-8408-d51dfaaf2f5a",
   "metadata": {},
   "source": [
    "On a réussi à passer de 1026 mots à 59 ! Il me faudrait maintenant une IA pour enlever les mots ayant moins d'intéret en fonction du sujet pour sélectionner les mots clé importants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8174ba8d-a792-43a9-a1dc-4069d4445431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(df['AB_AI']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cd6c4527-3b5e-4c32-8e3d-ebdb269c9629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ewenm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ewenm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ewenm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Faisons une fonction plus rapide pour sélectionner les mots importants d'un titre d'article : \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def est_nombre(w):\n",
    "        return re.fullmatch(r\"[0-9]+([.,][0-9]+)?\", w) is not None\n",
    "\n",
    "\n",
    "def mots_cles(text) : \n",
    "\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    sans_maj = [w.lower() for w in words]\n",
    "    words_filtered = []\n",
    "    for w in sans_maj:\n",
    "      if w not in stopwords:\n",
    "        words_filtered.append(w)\n",
    "\n",
    "\n",
    "#ponctuation \n",
    "    ponctuation = ['.',',',';','?',':','!','%']\n",
    "    words_sans_ponctuation=[x for x in words_filtered if x not in ponctuation]\n",
    "\n",
    "\n",
    "# doublons\n",
    "    W=list(set(words_sans_ponctuation))\n",
    "\n",
    "# sans les -ED\n",
    "    W_sans_ED = [w for w in W if not w.lower().endswith(\"ed\")] \n",
    "\n",
    "# singulier\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    W_singuliers = list(set([lemmatizer.lemmatize(w, pos='n') for w in W_sans_ED]))\n",
    "\n",
    "#sans les mots très courts\n",
    "    mots_assez_long = []\n",
    "    for w in W_singuliers : \n",
    "        if len(w)>=3 : \n",
    "            mots_assez_long.append(w)\n",
    "\n",
    "# sans les nombres \n",
    "   \n",
    "    W_sans_nombres = [w for w in mots_assez_long if not est_nombre(w)]\n",
    "\n",
    "    return(W_sans_nombres)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "12a8e23c-9b88-4d38-b51f-a9332de71b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de mots dans liste : 570379\n",
      "nombre de mots dans phrase :  18409\n"
     ]
    }
   ],
   "source": [
    "L = []\n",
    "\n",
    "#On vai maintenant parcourir tous les titres !\n",
    "for i in range (len(df['AB_AI'])) : \n",
    "    L.append(mots_cles(df['AB_AI'][i]))\n",
    "\n",
    "\n",
    "liste = [mot for sous_liste in L for mot in sous_liste]\n",
    "print('nombre de mots dans liste :',len(liste))  \n",
    "phrase = ' '.join(liste)\n",
    "\n",
    "print('nombre de mots dans phrase : ',len(mots_cles(phrase))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0734632b-7390-4583-96a8-03d120c3987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Je range dans un dictionnaire le mot et son nombre d'apparition dans l'ensemble des titres, et ce par ordre décroissant d'apparition.\n",
    "compteur = Counter(liste)\n",
    "dico_trie = dict(compteur.most_common())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3c77cca1-af8c-40c2-adce-9b89d5e09310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a réduit le nombre total de \"mots-clés\" de : 570379 à  234\n",
      "{'electric': 5740, 'vehicle': 5629, 'result': 3733, 'energy': 3708, 'system': 3623, 'study': 3277, 'model': 3163, 'paper': 3098, 'battery': 2693, 'charging': 2587, 'power': 2513, 'transport': 2298, 'using': 2278, 'electrification': 2077, 'different': 2068, 'emission': 2042, 'cost': 2026, 'also': 1998, 'show': 1981, 'analysis': 1940, 'however': 1855, 'impact': 1845, 'demand': 1806, 'technology': 1711, 'method': 1706, 'strategy': 1635, 'time': 1633, 'potential': 1578, 'performance': 1578, 'approach': 1546, 'data': 1524, 'use': 1479, 'fuel': 1471, 'increase': 1459, 'simulation': 1411, 'due': 1408, 'electricity': 1405, 'present': 1402, 'current': 1389, 'significant': 1387, 'high': 1379, 'carbon': 1376, 'reduce': 1364, 'network': 1308, 'sector': 1301, 'new': 1298, 'transportation': 1293, 'case': 1292, 'two': 1289, 'research': 1284, 'solution': 1282, 'scenario': 1260, 'operation': 1244, 'consumption': 1242, 'one': 1242, 'optimization': 1241, 'grid': 1230, 'efficiency': 1225, 'future': 1214, 'infrastructure': 1205, 'problem': 1182, 'development': 1172, 'environmental': 1171, 'challenge': 1156, 'management': 1152, 'effect': 1143, 'reduction': 1133, 'optimal': 1131, 'range': 1126, 'hybrid': 1119, 'renewable': 1110, 'level': 1109, 'policy': 1100, 'driving': 1091, 'condition': 1087, 'including': 1084, 'source': 1069, 'algorithm': 1069, 'load': 1063, 'control': 1061, 'change': 1047, 'station': 1043, 'gas': 1029, 'considering': 1025, 'capacity': 1020, 'factor': 1017, 'various': 1009, 'market': 999, 'provide': 986, 'design': 978, 'storage': 978, 'distribution': 973, 'well': 959, 'process': 940, 'dynamic': 934, 'application': 917, 'total': 913, 'number': 893, 'benefit': 888, 'could': 884, 'work': 881, 'generation': 874, 'charge': 871, 'area': 869, 'economic': 863, 'increasing': 862, 'important': 858, 'adoption': 846, 'framework': 844, 'aim': 842, 'conventional': 827, 'significantly': 825, 'three': 822, 'fleet': 819, 'year': 818, 'public': 818, 'first': 815, 'urban': 812, 'dioxide': 809, 'greenhouse': 801, 'issue': 800, 'per': 793, 'sustainable': 792, 'higher': 789, 'low': 788, 'parameter': 781, 'rate': 779, 'achieve': 777, 'global': 775, 'may': 774, 'finally': 763, 'behavior': 760, 'key': 759, 'transition': 759, 'region': 756, 'therefore': 746, 'finding': 744, 'within': 743, 'term': 741, 'state': 739, 'existing': 734, 'type': 731, 'mobility': 725, 'furthermore': 723, 'road': 722, 'cycle': 721, 'improve': 713, 'supply': 712, 'role': 712, 'characteristic': 704, 'novel': 700, 'service': 699, 'address': 697, 'main': 690, 'lower': 689, 'addition': 684, 'reducing': 679, 'value': 679, 'engine': 677, 'large': 674, 'price': 674, 'operating': 664, 'focus': 664, 'provides': 662, 'order': 659, 'proposes': 659, 'support': 657, 'test': 656, 'climate': 654, 'temperature': 647, 'several': 636, 'cell': 635, 'alternative': 633, 'planning': 630, 'internal': 628, 'modeling': 626, 'efficient': 621, 'mode': 619, 'industry': 617, 'air': 615, 'objective': 609, 'combustion': 608, 'environment': 605, 'production': 602, 'demonstrate': 598, 'effective': 591, 'moreover': 591, 'indicate': 590, 'without': 582, 'specific': 581, 'overall': 578, 'thus': 577, 'voltage': 576, 'resource': 575, 'influence': 570, 'among': 565, 'respectively': 564, 'integration': 558, 'peak': 558, 'set': 556, 'many': 553, 'user': 553, 'uncertainty': 551, 'direct': 549, 'less': 548, 'local': 545, 'electrical': 545, 'lead': 543, 'major': 539, 'bus': 538, 'available': 538, 'towards': 537, 'requirement': 534, 'effectiveness': 533, 'component': 532, 'plug-in': 529, 'better': 523, 'found': 517, 'target': 516, 'average': 516, 'point': 515, 'thermal': 511, 'operational': 510, 'heat': 509, 'location': 508, 'decision': 507, 'literature': 506, 'context': 506, 'passenger': 504, 'life': 504, 'goal': 503, 'smart': 502, 'flow': 501, 'comprehensive': 500}\n"
     ]
    }
   ],
   "source": [
    "# Je supprime tous les mots qui apparaissent moins de 500 fois en supposant ainsi que ce sont des mots peu importants, que je n'avais pas pu supprimer avant.\n",
    "\n",
    "compteur_filtre = {mot: nb for mot, nb in compteur.items() if nb >= 500}\n",
    "\n",
    "compteur_filtre_trie = dict(sorted(compteur_filtre.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print('On a réduit le nombre total de \"mots-clés\" de :',len(liste), \"à \",len(compteur_filtre_trie))\n",
    "print(compteur_filtre_trie)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098ef3b-6ecb-4269-b90c-694431176c7b",
   "metadata": {},
   "source": [
    "## En pure comparaison, j'ai demandé à chatGPT de me choisir les mots importants dans la liste L avec les 570000 mots : voici le résultat : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "10412772-1fb9-4156-824e-1fc7a061d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_filtré = {\n",
    "    'electric': 5740,\n",
    "    'vehicle': 5629,\n",
    "    'energy': 3708,\n",
    "    'system': 3624,\n",
    "    'study': 3278,\n",
    "    'model': 3164,\n",
    "    'battery': 2693,\n",
    "    'charging': 2587,\n",
    "    'power': 2513,\n",
    "    'transport': 2298,\n",
    "    'electrification': 2077,\n",
    "    'emission': 2042,\n",
    "    'cost': 2026,\n",
    "    'analysis': 1940,\n",
    "    'impact': 1845,\n",
    "    'demand': 1806,\n",
    "    'technology': 1711,\n",
    "    'method': 1708,\n",
    "    'strategy': 1635,\n",
    "    'performance': 1578,\n",
    "    'approach': 1546,\n",
    "    'data': 1524,\n",
    "    'fuel': 1471,\n",
    "    'simulation': 1411,\n",
    "    'electricity': 1405,\n",
    "    'carbon': 1376,\n",
    "    'reduce': 1364,\n",
    "    'network': 1310,\n",
    "    'sector': 1301,\n",
    "    'transportation': 1294,\n",
    "    'research': 1285,\n",
    "    'solution': 1282,\n",
    "    'operation': 1244,\n",
    "    'consumption': 1242,\n",
    "    'optimization': 1241,\n",
    "    'grid': 1230,\n",
    "    'efficiency': 1225,\n",
    "    'infrastructure': 1205,\n",
    "    'development': 1172,\n",
    "    'environmental': 1171,\n",
    "    'management': 1152,\n",
    "    'reduction': 1133,\n",
    "    'optimal': 1131,\n",
    "    'range': 1126,\n",
    "    'hybrid': 1119,\n",
    "    'renewable': 1110,\n",
    "    'policy': 1100,\n",
    "    'driving': 1091,\n",
    "    'source': 1069,\n",
    "    'algorithm': 1069,\n",
    "    'station': 1043,\n",
    "    'capacity': 1020,\n",
    "    'design': 978,\n",
    "    'storage': 978,\n",
    "    'distribution': 973,\n",
    "    'process': 940,\n",
    "    'dynamic': 934,\n",
    "    'application': 917,\n",
    "    'generation': 874,\n",
    "    'charge': 871,\n",
    "    'economic': 863,\n",
    "    'adoption': 846,\n",
    "    'framework': 844,\n",
    "    'conventional': 827,\n",
    "    'fleet': 819,\n",
    "    'public': 818,\n",
    "    'urban': 812,\n",
    "    'dioxide': 809,\n",
    "    'greenhouse': 801,\n",
    "    'sustainable': 792,\n",
    "    'parameter': 781,\n",
    "    'transition': 760,\n",
    "    'behavior': 760,\n",
    "    'key': 759,\n",
    "    'mobility': 725,\n",
    "    'cycle': 721,\n",
    "    'supply': 712,\n",
    "    'characteristic': 704,\n",
    "    'service': 699,\n",
    "    'reducing': 679,\n",
    "    'engine': 677,\n",
    "    'operating': 664,\n",
    "    'climate': 654,\n",
    "    'cell': 635,\n",
    "    'alternative': 633,\n",
    "    'planning': 630,\n",
    "    'internal': 628,\n",
    "    'modeling': 626,\n",
    "    'efficient': 621,\n",
    "    'mode': 619,\n",
    "    'industry': 618,\n",
    "    'air': 615,\n",
    "    'objective': 609,\n",
    "    'combustion': 608,\n",
    "    'environment': 605,\n",
    "    'production': 602,\n",
    "    'effective': 591,\n",
    "    'integration': 559,\n",
    "    'electrical': 545,\n",
    "    'bus': 538,\n",
    "    'smart': 502,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "19e97b1d-6286-4065-9d28-fafadbdedbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "print(len(dico_filtré)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263a885-bd99-43e6-a011-92c4b98394c3",
   "metadata": {},
   "source": [
    "On passe de 234 mots avec ma méthode à 101 mots avec chatGPT. Sur 570 000 mots d'origine, je considère que ma méthode est plutôt satisfaisante, surtout que les 101 mots sont bien compris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6c95aaa3-efca-44c2-be4c-027dfd589d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'electric': 100.0, 'vehicle': 98.09, 'energy': 65.09, 'system': 63.64, 'study': 57.7, 'model': 55.74, 'battery': 47.65, 'charging': 45.82, 'power': 44.55, 'transport': 40.86, 'electrification': 37.06, 'emission': 36.46, 'cost': 36.19, 'analysis': 34.71, 'impact': 33.08, 'demand': 32.41, 'technology': 30.77, 'method': 30.72, 'strategy': 29.47, 'performance': 28.49, 'approach': 27.94, 'data': 27.56, 'fuel': 26.65, 'simulation': 25.62, 'electricity': 25.52, 'carbon': 25.02, 'reduce': 24.81, 'network': 23.88, 'sector': 23.73, 'transportation': 23.61, 'research': 23.45, 'solution': 23.4, 'operation': 22.75, 'consumption': 22.71, 'optimization': 22.7, 'grid': 22.51, 'efficiency': 22.42, 'infrastructure': 22.08, 'development': 21.51, 'environmental': 21.49, 'management': 21.17, 'reduction': 20.84, 'optimal': 20.81, 'range': 20.72, 'hybrid': 20.6, 'renewable': 20.45, 'policy': 20.27, 'driving': 20.12, 'source': 19.74, 'algorithm': 19.74, 'station': 19.3, 'capacity': 18.9, 'design': 18.18, 'storage': 18.18, 'distribution': 18.09, 'process': 17.53, 'dynamic': 17.42, 'application': 17.13, 'generation': 16.39, 'charge': 16.34, 'economic': 16.2, 'adoption': 15.91, 'framework': 15.88, 'conventional': 15.58, 'fleet': 15.45, 'public': 15.43, 'urban': 15.33, 'dioxide': 15.27, 'greenhouse': 15.14, 'sustainable': 14.98, 'parameter': 14.79, 'transition': 14.43, 'behavior': 14.43, 'key': 14.42, 'mobility': 13.83, 'cycle': 13.76, 'supply': 13.61, 'characteristic': 13.47, 'service': 13.38, 'reducing': 13.04, 'engine': 13.01, 'operating': 12.78, 'climate': 12.61, 'cell': 12.29, 'alternative': 12.25, 'planning': 12.2, 'internal': 12.16, 'modeling': 12.13, 'efficient': 12.04, 'mode': 12.01, 'industry': 11.99, 'air': 11.94, 'objective': 11.84, 'combustion': 11.82, 'environment': 11.77, 'production': 11.72, 'effective': 11.53, 'integration': 10.98, 'electrical': 10.74, 'bus': 10.62, 'smart': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# On veut maintenant calculer des poids d'importance des mots pour qu'on puisse attribuer à chaque mot un rayon plus ou moins grand. \n",
    "# On fait une echelle pour avoir des rayons compris entre 10 et 100. \n",
    "def scale(freq, min_freq, max_freq, min_scale=10, max_scale=100):\n",
    "    return min_scale + (freq - min_freq) * (max_scale - min_scale) / (max_freq - min_freq)\n",
    "\n",
    "min_freq = min(dico_filtré.values())\n",
    "max_freq = max(dico_filtré.values())\n",
    "\n",
    "rayon_mots = {\n",
    "    mot: round(scale(freq, min_freq, max_freq), 2)\n",
    "    for mot, freq in dico_filtré.items()\n",
    "}\n",
    "\n",
    "print(rayon_mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5d52f302-b126-438d-af66-7c8f8d8dc09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 101)\n",
      "          electric  vehicle  energy  system  study  model  battery  charging  \\\n",
      "electric         0     5249    2970    2936   2721   2667     2454      2469   \n",
      "vehicle       5249        0    2916    2853   2653   2612     2359      2355   \n",
      "energy        2970     2916       0    2220   1699   1672     1551      1346   \n",
      "system        2936     2853    2220       0   1660   1661     1448      1383   \n",
      "study         2721     2653    1699    1660      0   1552     1265      1253   \n",
      "\n",
      "          power  transport  ...  air  objective  combustion  environment  \\\n",
      "electric   2124       1555  ...  486        534         542          493   \n",
      "vehicle    2022       1465  ...  490        521         555          485   \n",
      "energy     1669       1312  ...  333        378         321          352   \n",
      "system     1679       1191  ...  324        356         226          326   \n",
      "study      1078       1059  ...  281        302         302          280   \n",
      "\n",
      "          production  effective  integration  electrical  bus  smart  \n",
      "electric         435        510          471         412  474    456  \n",
      "vehicle          429        495          461         386  372    440  \n",
      "energy           406        310          404         350  302    360  \n",
      "system           295        312          411         352  307    329  \n",
      "study            293        297          262         219  263    238  \n",
      "\n",
      "[5 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "tous_mots = list(dico_filtré)\n",
    "\n",
    "cooc_matrice = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# On parcourt chaque sous-liste de mots (chaque titre)\n",
    "for mots in L:\n",
    "    mots_filtres = [m for m in set(mots) if m in tous_mots] \n",
    "    for w1, w2 in itertools.combinations(mots_filtres, 2):\n",
    "        cooc_matrice[w1][w2] += 1\n",
    "        cooc_matrice[w2][w1] += 1  #comme on aura une matrice symétrique, on essaie de réduire la complexité en ne parcourant pas deux fois\n",
    "        \n",
    "df_cooc = pd.DataFrame(cooc_matrice, index=tous_mots, columns=tous_mots).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "print(df_cooc.shape)  \n",
    "print(df_cooc.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f93d225f-6f45-466e-aa6d-2aa0e154967b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 liens trouvés avec cooccurrence > 1000\n",
      "[('electric', 'vehicle'), ('electric', 'energy'), ('electric', 'system'), ('electric', 'study'), ('electric', 'model'), ('electric', 'battery'), ('electric', 'charging'), ('electric', 'power'), ('electric', 'transport'), ('electric', 'electrification'), ('electric', 'emission'), ('electric', 'cost'), ('electric', 'analysis'), ('electric', 'impact'), ('electric', 'demand'), ('electric', 'technology'), ('electric', 'method'), ('electric', 'strategy'), ('electric', 'performance'), ('electric', 'approach'), ('electric', 'data'), ('electric', 'fuel'), ('electric', 'simulation'), ('electric', 'electricity'), ('electric', 'carbon'), ('electric', 'reduce'), ('electric', 'network'), ('electric', 'transportation'), ('electric', 'research'), ('electric', 'solution'), ('electric', 'operation'), ('electric', 'consumption'), ('electric', 'optimization'), ('electric', 'grid'), ('electric', 'infrastructure'), ('electric', 'management'), ('electric', 'optimal'), ('electric', 'hybrid'), ('vehicle', 'energy'), ('vehicle', 'system'), ('vehicle', 'study'), ('vehicle', 'model'), ('vehicle', 'battery'), ('vehicle', 'charging'), ('vehicle', 'power'), ('vehicle', 'transport'), ('vehicle', 'electrification'), ('vehicle', 'emission'), ('vehicle', 'cost'), ('vehicle', 'analysis'), ('vehicle', 'impact'), ('vehicle', 'demand'), ('vehicle', 'technology'), ('vehicle', 'method'), ('vehicle', 'strategy'), ('vehicle', 'performance'), ('vehicle', 'approach'), ('vehicle', 'data'), ('vehicle', 'fuel'), ('vehicle', 'simulation'), ('vehicle', 'electricity'), ('vehicle', 'carbon'), ('vehicle', 'reduce'), ('vehicle', 'network'), ('vehicle', 'transportation'), ('vehicle', 'research'), ('vehicle', 'solution'), ('vehicle', 'consumption'), ('vehicle', 'optimization'), ('vehicle', 'grid'), ('vehicle', 'infrastructure'), ('vehicle', 'management'), ('vehicle', 'hybrid'), ('vehicle', 'driving'), ('energy', 'system'), ('energy', 'study'), ('energy', 'model'), ('energy', 'battery'), ('energy', 'charging'), ('energy', 'power'), ('energy', 'transport'), ('energy', 'electrification'), ('energy', 'emission'), ('energy', 'cost'), ('energy', 'analysis'), ('energy', 'impact'), ('energy', 'demand'), ('energy', 'technology'), ('energy', 'electricity'), ('energy', 'consumption'), ('energy', 'renewable'), ('system', 'study'), ('system', 'model'), ('system', 'battery'), ('system', 'charging'), ('system', 'power'), ('system', 'transport'), ('system', 'electrification'), ('system', 'cost'), ('system', 'demand'), ('study', 'model'), ('study', 'battery'), ('study', 'charging'), ('study', 'power'), ('study', 'transport'), ('study', 'emission'), ('study', 'analysis'), ('model', 'battery'), ('model', 'charging'), ('model', 'power'), ('model', 'cost'), ('battery', 'charging'), ('battery', 'power'), ('charging', 'power'), ('transport', 'electrification'), ('emission', 'carbon')]\n"
     ]
    }
   ],
   "source": [
    "### Liens entre les bulles de mots \n",
    "\n",
    "liens= []\n",
    "\n",
    "for a, b in itertools.combinations(df_cooc.columns, 2):\n",
    "    if df_cooc.loc[a, b] > 1000:  # on choisi de garder que les mots que reviennent ensemble de les titres plus de 1000 fois.\n",
    "        liens.append((a, b))\n",
    "\n",
    "print(f\"{len(liens)} liens trouvés avec cooccurrence > 1000\")\n",
    "print(liens)  # aperçu des premières paires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "58d9f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "for x in dico_filtré.keys():\n",
    "    products.append({\n",
    "        \"id\": x,\n",
    "        \"group\": 1,\n",
    "        \"value\": rayon_mots[x]  # rayon du mot\n",
    "    })\n",
    "\n",
    "edges = liens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "96be3cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x18926d58e00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dash import State\n",
    "\n",
    "# Création du graphe\n",
    "G = nx.Graph()\n",
    "for product in products:\n",
    "    G.add_node(product[\"id\"], group=product[\"group\"], value=product[\"value\"])\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Positions\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Infos nœuds\n",
    "node_x, node_y, node_text, node_color, node_size = [], [], [], [], []\n",
    "color_map = {\"Austria\":  \"#FB3131\", \"France\": \"#50C6EE\", \"Peru\": \"#F2A900\"}\n",
    "\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)\n",
    "    node_color.append(color_map.get(G.nodes[node][\"group\"], \"grey\"))\n",
    "    node_size.append(G.nodes[node][\"value\"])\n",
    "\n",
    "# Trace des nœuds\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x,\n",
    "    y=node_y,\n",
    "    mode=\"markers+text\",\n",
    "    text=node_text,\n",
    "    hoverinfo=\"text\",\n",
    "    customdata=node_text,\n",
    "    marker=dict(showscale=False, color=\"grey\", size=node_size, line_width=2),\n",
    ")\n",
    "\n",
    "# App Dash\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div(\n",
    "    [\n",
    "        html.H1(\"Product space\"),\n",
    "        html.P(\"Select country:\"),\n",
    "        dcc.Dropdown(\n",
    "            id=\"dropdown\",\n",
    "            options=[\"Austria\", \"France\", \"Peru\"],\n",
    "            value=\"Austria\",\n",
    "            clearable=False,\n",
    "        ),\n",
    "        dcc.Graph(\n",
    "            id=\"product-space\",\n",
    "            figure=go.Figure(\n",
    "                data=[node_trace],\n",
    "                layout=go.Layout(\n",
    "                    showlegend=False,\n",
    "                    hovermode=\"closest\",\n",
    "                    margin=dict(b=20, l=5, r=5, t=40),\n",
    "                    xaxis=dict(showgrid=False, zeroline=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False),\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"product-space\", \"figure\"),\n",
    "    Input(\"product-space\", \"hoverData\"),\n",
    "    State(\"product-space\", \"relayoutData\"),\n",
    "    State(\"dropdown\", \"value\"),\n",
    ")\n",
    "def update_graph(hoverData, relayoutData, country):\n",
    "    fig = go.Figure(\n",
    "            data=[node_trace],\n",
    "            layout=go.Layout(\n",
    "                showlegend=False,\n",
    "                hovermode=\"closest\",\n",
    "                margin=dict(b=20, l=5, r=5, t=40),\n",
    "                xaxis=dict(showgrid=False, zeroline=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False),\n",
    "            ),\n",
    "        ) \n",
    "    if relayoutData:\n",
    "        if 'xaxis.range[0]' in relayoutData and 'xaxis.range[1]' in relayoutData:\n",
    "            fig.update_xaxes(range=[\n",
    "                relayoutData['xaxis.range[0]'],\n",
    "                relayoutData['xaxis.range[1]']\n",
    "            ])\n",
    "        if 'yaxis.range[0]' in relayoutData and 'yaxis.range[1]' in relayoutData:\n",
    "            fig.update_yaxes(range=[\n",
    "                relayoutData['yaxis.range[0]'],\n",
    "                relayoutData['yaxis.range[1]']\n",
    "            ])\n",
    "\n",
    "    if hoverData is None:\n",
    "        return fig \n",
    "\n",
    "    # Récupérer le nœud survolé\n",
    "    try:\n",
    "        node_id = hoverData[\"points\"][0][\"customdata\"]\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        # Fallback en cas de structure inattendue\n",
    "        return dash.no_update\n",
    "\n",
    "    # Arêtes liées à ce nœud\n",
    "    filtered_edges = [e for e in G.edges() if node_id in e]\n",
    "\n",
    "    # Générer uniquement les segments des arêtes concernées\n",
    "    edge_x, edge_y = [], []\n",
    "    for edge in filtered_edges:\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x += [x0, x1, None]\n",
    "        edge_y += [y0, y1, None]\n",
    "\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        line=dict(width=2, color=\"#888\"),\n",
    "        hoverinfo=\"none\",\n",
    "        mode=\"lines\",\n",
    "    )\n",
    "    if node_size[node_text.index(node_id)] >= 20:\n",
    "        col = color_map[country] if country in color_map else node_color[node_text.index(node_id)]\n",
    "    else:\n",
    "        col = \"grey\"\n",
    "\n",
    "    color_trace = go.Scatter(\n",
    "        x=[pos[node_id][0]],\n",
    "        y=[pos[node_id][1]],\n",
    "        mode=\"markers + text\",\n",
    "        text=node_text[node_text.index(node_id)],\n",
    "        hoverinfo=\"text\",\n",
    "        marker=dict(\n",
    "            color=col,\n",
    "            size=node_size[node_text.index(node_id)],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(edge_trace)\n",
    "    fig.add_trace(color_trace)\n",
    "\n",
    "    return fig \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2a6f218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   topic_0   topic_1   topic_2   topic_3   topic_4   topic_5  \\\n",
      "0     topic_0  1.000000  0.763636  0.690909  0.709091  0.709091  0.600000   \n",
      "1     topic_1  0.763636  1.000000  0.672727  0.636364  0.781818  0.654545   \n",
      "2     topic_2  0.690909  0.672727  1.000000  0.680000  0.686275  0.620000   \n",
      "3     topic_3  0.709091  0.636364  0.680000  1.000000  0.686275  0.583333   \n",
      "4     topic_4  0.709091  0.781818  0.686275  0.686275  1.000000  0.647059   \n",
      "5     topic_5  0.600000  0.654545  0.620000  0.583333  0.647059  1.000000   \n",
      "6     topic_6  0.690909  0.709091  0.760000  0.693878  0.705882  0.653061   \n",
      "7     topic_7  0.709091  0.672727  0.711538  0.692308  0.711538  0.615385   \n",
      "8     topic_8  0.490909  0.472727  0.520000  0.541667  0.509804  0.452381   \n",
      "9     topic_9  0.436364  0.400000  0.480000  0.416667  0.431373  0.380952   \n",
      "10   topic_10  0.690909  0.672727  0.720000  0.666667  0.666667  0.687500   \n",
      "11   topic_11  0.418182  0.363636  0.380000  0.395833  0.411765  0.404762   \n",
      "12   topic_12  0.618182  0.636364  0.660000  0.620000  0.666667  0.700000   \n",
      "13   topic_13  0.636364  0.709091  0.700000  0.645833  0.666667  0.644444   \n",
      "14   topic_14  0.563636  0.563636  0.600000  0.604167  0.568627  0.642857   \n",
      "15   topic_15  0.454545  0.472727  0.480000  0.500000  0.490196  0.500000   \n",
      "16   topic_16  0.600000  0.618182  0.640000  0.666667  0.647059  0.690476   \n",
      "17   topic_17  0.563636  0.563636  0.520000  0.583333  0.568627  0.642857   \n",
      "18   topic_18  0.436364  0.490909  0.500000  0.479167  0.509804  0.500000   \n",
      "19   topic_19  0.490909  0.509091  0.480000  0.500000  0.529412  0.571429   \n",
      "20   topic_20  0.490909  0.472727  0.480000  0.520833  0.490196  0.476190   \n",
      "21   topic_21  0.509091  0.563636  0.520000  0.520833  0.549020  0.571429   \n",
      "22   topic_22  0.400000  0.436364  0.400000  0.416667  0.450980  0.500000   \n",
      "23   topic_23  0.490909  0.527273  0.540000  0.520833  0.529412  0.547619   \n",
      "24   topic_24  0.418182  0.454545  0.460000  0.437500  0.509804  0.500000   \n",
      "25   topic_25  0.309091  0.272727  0.320000  0.291667  0.294118  0.309524   \n",
      "26   topic_26  0.454545  0.381818  0.460000  0.458333  0.411765  0.476190   \n",
      "27   topic_27  0.509091  0.545455  0.540000  0.458333  0.568627  0.619048   \n",
      "28   topic_28  0.436364  0.454545  0.480000  0.458333  0.431373  0.452381   \n",
      "29   topic_29  0.490909  0.509091  0.460000  0.437500  0.490196  0.547619   \n",
      "30   topic_30  0.418182  0.400000  0.460000  0.416667  0.392157  0.404762   \n",
      "31   topic_31  0.436364  0.454545  0.480000  0.458333  0.470588  0.523810   \n",
      "32   topic_32  0.472727  0.454545  0.480000  0.520833  0.470588  0.523810   \n",
      "33   topic_33  0.472727  0.472727  0.480000  0.437500  0.490196  0.571429   \n",
      "34   topic_34  0.454545  0.454545  0.400000  0.479167  0.431373  0.547619   \n",
      "35   topic_35  0.363636  0.345455  0.420000  0.416667  0.352941  0.333333   \n",
      "36   topic_36  0.436364  0.418182  0.460000  0.458333  0.450980  0.523810   \n",
      "37   topic_37  0.345455  0.418182  0.400000  0.375000  0.392157  0.452381   \n",
      "38   topic_38  0.290909  0.345455  0.320000  0.312500  0.352941  0.404762   \n",
      "39   topic_39  0.345455  0.327273  0.320000  0.354167  0.372549  0.380952   \n",
      "40   topic_40  0.363636  0.345455  0.400000  0.416667  0.372549  0.404762   \n",
      "41   topic_41  0.363636  0.363636  0.360000  0.395833  0.333333  0.404762   \n",
      "42   topic_42  0.363636  0.345455  0.380000  0.375000  0.333333  0.333333   \n",
      "43   topic_43  0.272727  0.272727  0.280000  0.270833  0.294118  0.357143   \n",
      "44   topic_44  0.254545  0.236364  0.280000  0.312500  0.254902  0.285714   \n",
      "45   topic_45  0.200000  0.181818  0.240000  0.229167  0.176471  0.166667   \n",
      "46   topic_46  0.218182  0.218182  0.240000  0.229167  0.254902  0.261905   \n",
      "47   topic_47  0.109091  0.109091  0.120000  0.104167  0.098039  0.119048   \n",
      "48   topic_48  0.072727  0.054545  0.080000  0.083333  0.058824  0.095238   \n",
      "49   topic_49  0.072727  0.072727  0.080000  0.083333  0.078431  0.119048   \n",
      "\n",
      "     topic_6   topic_7   topic_8  ...  topic_40  topic_41  topic_42  topic_43  \\\n",
      "0   0.690909  0.709091  0.490909  ...  0.363636  0.363636  0.363636  0.272727   \n",
      "1   0.709091  0.672727  0.472727  ...  0.345455  0.363636  0.345455  0.272727   \n",
      "2   0.760000  0.711538  0.520000  ...  0.400000  0.360000  0.380000  0.280000   \n",
      "3   0.693878  0.692308  0.541667  ...  0.416667  0.395833  0.375000  0.270833   \n",
      "4   0.705882  0.711538  0.509804  ...  0.372549  0.333333  0.333333  0.294118   \n",
      "5   0.653061  0.615385  0.452381  ...  0.404762  0.404762  0.333333  0.357143   \n",
      "6   1.000000  0.653846  0.571429  ...  0.346939  0.387755  0.367347  0.285714   \n",
      "7   0.653846  1.000000  0.480769  ...  0.365385  0.365385  0.326923  0.269231   \n",
      "8   0.571429  0.480769  1.000000  ...  0.562500  0.468750  0.437500  0.250000   \n",
      "9   0.448980  0.423077  0.593750  ...  0.629630  0.407407  0.555556  0.333333   \n",
      "10  0.734694  0.730769  0.479167  ...  0.395833  0.375000  0.354167  0.291667   \n",
      "11  0.428571  0.384615  0.593750  ...  0.538462  0.461538  0.500000  0.307692   \n",
      "12  0.660000  0.653846  0.420000  ...  0.320000  0.360000  0.300000  0.280000   \n",
      "13  0.714286  0.653846  0.533333  ...  0.422222  0.377778  0.400000  0.266667   \n",
      "14  0.591837  0.557692  0.583333  ...  0.527778  0.444444  0.444444  0.388889   \n",
      "15  0.489796  0.500000  0.687500  ...  0.500000  0.468750  0.468750  0.312500   \n",
      "16  0.673469  0.634615  0.657895  ...  0.500000  0.447368  0.421053  0.289474   \n",
      "17  0.632653  0.576923  0.583333  ...  0.388889  0.444444  0.416667  0.361111   \n",
      "18  0.489796  0.442308  0.625000  ...  0.586207  0.482759  0.413793  0.413793   \n",
      "19  0.612245  0.461538  0.705882  ...  0.441176  0.411765  0.470588  0.323529   \n",
      "20  0.530612  0.480769  0.593750  ...  0.466667  0.533333  0.433333  0.366667   \n",
      "21  0.591837  0.480769  0.625000  ...  0.468750  0.468750  0.468750  0.468750   \n",
      "22  0.448980  0.365385  0.500000  ...  0.480000  0.520000  0.520000  0.280000   \n",
      "23  0.530612  0.461538  0.625000  ...  0.531250  0.437500  0.531250  0.406250   \n",
      "24  0.469388  0.442308  0.500000  ...  0.517241  0.413793  0.448276  0.379310   \n",
      "25  0.306122  0.269231  0.437500  ...  0.571429  0.454545  0.545455  0.411765   \n",
      "26  0.489796  0.461538  0.562500  ...  0.500000  0.464286  0.535714  0.357143   \n",
      "27  0.530612  0.480769  0.428571  ...  0.428571  0.457143  0.485714  0.342857   \n",
      "28  0.510204  0.403846  0.562500  ...  0.518519  0.518519  0.481481  0.444444   \n",
      "29  0.489796  0.442308  0.593750  ...  0.387097  0.387097  0.483871  0.354839   \n",
      "30  0.408163  0.403846  0.531250  ...  0.708333  0.500000  0.583333  0.333333   \n",
      "31  0.489796  0.461538  0.562500  ...  0.466667  0.433333  0.433333  0.333333   \n",
      "32  0.530612  0.423077  0.593750  ...  0.466667  0.466667  0.500000  0.333333   \n",
      "33  0.489796  0.442308  0.468750  ...  0.343750  0.437500  0.468750  0.375000   \n",
      "34  0.469388  0.403846  0.437500  ...  0.419355  0.516129  0.387097  0.451613   \n",
      "35  0.408163  0.403846  0.562500  ...  0.541667  0.416667  0.541667  0.250000   \n",
      "36  0.489796  0.461538  0.562500  ...  0.535714  0.428571  0.535714  0.357143   \n",
      "37  0.408163  0.423077  0.406250  ...  0.360000  0.520000  0.440000  0.320000   \n",
      "38  0.367347  0.326923  0.406250  ...  0.523810  0.500000  0.545455  0.400000   \n",
      "39  0.346939  0.365385  0.437500  ...  0.571429  0.590909  0.409091  0.333333   \n",
      "40  0.346939  0.365385  0.562500  ...  1.000000  0.545455  0.500000  0.380952   \n",
      "41  0.387755  0.365385  0.468750  ...  0.545455  1.000000  0.500000  0.363636   \n",
      "42  0.367347  0.326923  0.437500  ...  0.500000  0.500000  1.000000  0.318182   \n",
      "43  0.285714  0.269231  0.250000  ...  0.380952  0.363636  0.318182  1.000000   \n",
      "44  0.326531  0.288462  0.343750  ...  0.571429  0.272727  0.363636  0.315789   \n",
      "45  0.204082  0.211538  0.281250  ...  0.380952  0.272727  0.318182  0.235294   \n",
      "46  0.265306  0.192308  0.343750  ...  0.380952  0.363636  0.454545  0.470588   \n",
      "47  0.122449  0.096154  0.156250  ...  0.238095  0.272727  0.227273  0.294118   \n",
      "48  0.081633  0.076923  0.125000  ...  0.190476  0.136364  0.090909  0.176471   \n",
      "49  0.081633  0.076923  0.093750  ...  0.142857  0.136364  0.181818  0.117647   \n",
      "\n",
      "    topic_44  topic_45  topic_46  topic_47  topic_48  topic_49  \n",
      "0   0.254545  0.200000  0.218182  0.109091  0.072727  0.072727  \n",
      "1   0.236364  0.181818  0.218182  0.109091  0.054545  0.072727  \n",
      "2   0.280000  0.240000  0.240000  0.120000  0.080000  0.080000  \n",
      "3   0.312500  0.229167  0.229167  0.104167  0.083333  0.083333  \n",
      "4   0.254902  0.176471  0.254902  0.098039  0.058824  0.078431  \n",
      "5   0.285714  0.166667  0.261905  0.119048  0.095238  0.119048  \n",
      "6   0.326531  0.204082  0.265306  0.122449  0.081633  0.081633  \n",
      "7   0.288462  0.211538  0.192308  0.096154  0.076923  0.076923  \n",
      "8   0.343750  0.281250  0.343750  0.156250  0.125000  0.093750  \n",
      "9   0.407407  0.333333  0.370370  0.148148  0.148148  0.111111  \n",
      "10  0.312500  0.166667  0.229167  0.125000  0.083333  0.083333  \n",
      "11  0.384615  0.384615  0.384615  0.153846  0.115385  0.115385  \n",
      "12  0.240000  0.160000  0.240000  0.100000  0.080000  0.100000  \n",
      "13  0.333333  0.244444  0.244444  0.133333  0.044444  0.088889  \n",
      "14  0.361111  0.250000  0.305556  0.138889  0.083333  0.083333  \n",
      "15  0.312500  0.281250  0.375000  0.156250  0.125000  0.093750  \n",
      "16  0.315789  0.263158  0.289474  0.131579  0.105263  0.105263  \n",
      "17  0.305556  0.166667  0.305556  0.138889  0.083333  0.083333  \n",
      "18  0.310345  0.310345  0.379310  0.172414  0.137931  0.103448  \n",
      "19  0.382353  0.264706  0.323529  0.147059  0.088235  0.117647  \n",
      "20  0.300000  0.233333  0.333333  0.166667  0.100000  0.133333  \n",
      "21  0.312500  0.250000  0.375000  0.156250  0.093750  0.062500  \n",
      "22  0.400000  0.240000  0.360000  0.120000  0.120000  0.120000  \n",
      "23  0.343750  0.281250  0.312500  0.187500  0.093750  0.125000  \n",
      "24  0.344828  0.206897  0.344828  0.172414  0.103448  0.137931  \n",
      "25  0.473684  0.294118  0.411765  0.235294  0.176471  0.117647  \n",
      "26  0.464286  0.321429  0.357143  0.142857  0.107143  0.142857  \n",
      "27  0.285714  0.200000  0.285714  0.171429  0.085714  0.114286  \n",
      "28  0.333333  0.370370  0.407407  0.148148  0.111111  0.074074  \n",
      "29  0.290323  0.161290  0.322581  0.193548  0.064516  0.129032  \n",
      "30  0.500000  0.375000  0.333333  0.208333  0.166667  0.166667  \n",
      "31  0.400000  0.300000  0.300000  0.166667  0.100000  0.133333  \n",
      "32  0.300000  0.300000  0.333333  0.166667  0.100000  0.100000  \n",
      "33  0.250000  0.187500  0.343750  0.156250  0.062500  0.125000  \n",
      "34  0.290323  0.258065  0.322581  0.161290  0.096774  0.096774  \n",
      "35  0.458333  0.375000  0.375000  0.083333  0.166667  0.125000  \n",
      "36  0.392857  0.250000  0.392857  0.142857  0.142857  0.107143  \n",
      "37  0.320000  0.240000  0.280000  0.120000  0.080000  0.120000  \n",
      "38  0.350000  0.200000  0.450000  0.200000  0.100000  0.150000  \n",
      "39  0.476190  0.238095  0.333333  0.190476  0.095238  0.142857  \n",
      "40  0.571429  0.380952  0.380952  0.238095  0.190476  0.142857  \n",
      "41  0.272727  0.272727  0.363636  0.272727  0.136364  0.136364  \n",
      "42  0.363636  0.318182  0.454545  0.227273  0.090909  0.181818  \n",
      "43  0.315789  0.235294  0.470588  0.294118  0.176471  0.117647  \n",
      "44  1.000000  0.368421  0.210526  0.157895  0.210526  0.157895  \n",
      "45  0.368421  1.000000  0.285714  0.071429  0.214286  0.071429  \n",
      "46  0.210526  0.285714  1.000000  0.307692  0.153846  0.153846  \n",
      "47  0.157895  0.071429  0.307692  1.000000  0.166667  0.333333  \n",
      "48  0.210526  0.214286  0.153846  0.166667  1.000000  0.000000  \n",
      "49  0.157895  0.071429  0.153846  0.333333  0.000000  1.000000  \n",
      "\n",
      "[50 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"Proximity_Matrix.csv\", sep=';')\n",
    "\n",
    "topics = df2.columns[1:]  # Exclure la première colonne qui contient les noms des topics\n",
    "clusters = [['topic_47', 'topic_49'],['topic_16','topic_15', 'topic_18', 'topic_19', 'topic_9', 'topic_35', 'topic_45','topic_42','topic_11', 'topic_40'],['topic_25', 'topic_8', 'topic_46'],['topic_22', 'topic_37'],['topic_34','topic_29','topic_4','topic_20','topic_0','topic_43','topic_7','topic_10', 'topic_21', 'topic_12', 'topic_17', 'topic_33','topic_1','topic_31', 'topic_38','topic_41', 'topic_27','topic_24'],['topic_5','topic_32','topic_26','topic_13', 'topic_14','topic_39', 'topic_36', 'topic_28', 'topic_3', 'topic_2', 'topic_6', 'topic_23', 'topic_30', 'topic_44', 'topic_48']]\n",
    "\n",
    "edges2 = []\n",
    "n = len(topics)\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7b239e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_topic(topic, clusters):\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        if topic in cluster:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "subjects = []\n",
    "for topic in topics : \n",
    "    subjects.append({\n",
    "        \"id\": topic,\n",
    "        \"group\": index_of_topic(topic, clusters),  # Groupe basé sur le cluster auquel appartient le topic\n",
    "        \"value\": 10\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4c5fee8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m     node_y\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[0;32m     18\u001b[0m     node_text\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[1;32m---> 19\u001b[0m     node_color\u001b[38;5;241m.\u001b[39mappend(color_map\u001b[38;5;241m.\u001b[39mget(\u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrey\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     20\u001b[0m     node_size\u001b[38;5;241m.\u001b[39mappend(G\u001b[38;5;241m.\u001b[39mnodes[node][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Trace des nœuds\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'group'"
     ]
    }
   ],
   "source": [
    "# Création du graphe\n",
    "G = nx.Graph()\n",
    "for subject in subjects:\n",
    "    G.add_node(subject[\"id\"], group=subject[\"group\"], value=subject[\"value\"])\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Positions\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Infos nœuds\n",
    "node_x, node_y, node_text, node_color, node_size = [], [], [], [], []\n",
    "color_map = {\"Austria\":  \"#FB3131\", \"France\": \"#50C6EE\", \"Peru\": \"#F2A900\"}\n",
    "\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)\n",
    "    node_color.append(color_map.get(G.nodes[node][\"group\"], \"grey\"))\n",
    "    node_size.append(G.nodes[node][\"value\"])\n",
    "\n",
    "# Trace des nœuds\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x,\n",
    "    y=node_y,\n",
    "    mode=\"markers+text\",\n",
    "    text=node_text,\n",
    "    hoverinfo=\"text\",\n",
    "    customdata=node_text,\n",
    "    marker=dict(showscale=False, color=\"grey\", size=node_size, line_width=2),\n",
    ")\n",
    "\n",
    "# App Dash\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div(\n",
    "    [\n",
    "        html.H1(\"Product space\"),\n",
    "        html.P(\"Select country:\"),\n",
    "        dcc.Dropdown(\n",
    "            id=\"dropdown\",\n",
    "            options=[\"Austria\", \"France\", \"Peru\"],\n",
    "            value=\"Austria\",\n",
    "            clearable=False,\n",
    "        ),\n",
    "        dcc.Graph(\n",
    "            id=\"product-space\",\n",
    "            figure=go.Figure(\n",
    "                data=[node_trace],\n",
    "                layout=go.Layout(\n",
    "                    showlegend=False,\n",
    "                    hovermode=\"closest\",\n",
    "                    margin=dict(b=20, l=5, r=5, t=40),\n",
    "                    xaxis=dict(showgrid=False, zeroline=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False),\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"product-space\", \"figure\"),\n",
    "    Input(\"product-space\", \"hoverData\"),\n",
    "    State(\"product-space\", \"relayoutData\"),\n",
    "    State(\"dropdown\", \"value\"),\n",
    ")\n",
    "def update_graph(hoverData, relayoutData, country):\n",
    "    fig = go.Figure(\n",
    "            data=[node_trace],\n",
    "            layout=go.Layout(\n",
    "                showlegend=False,\n",
    "                hovermode=\"closest\",\n",
    "                margin=dict(b=20, l=5, r=5, t=40),\n",
    "                xaxis=dict(showgrid=False, zeroline=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False),\n",
    "            ),\n",
    "        ) \n",
    "    if relayoutData:\n",
    "        if 'xaxis.range[0]' in relayoutData and 'xaxis.range[1]' in relayoutData:\n",
    "            fig.update_xaxes(range=[\n",
    "                relayoutData['xaxis.range[0]'],\n",
    "                relayoutData['xaxis.range[1]']\n",
    "            ])\n",
    "        if 'yaxis.range[0]' in relayoutData and 'yaxis.range[1]' in relayoutData:\n",
    "            fig.update_yaxes(range=[\n",
    "                relayoutData['yaxis.range[0]'],\n",
    "                relayoutData['yaxis.range[1]']\n",
    "            ])\n",
    "\n",
    "    if hoverData is None:\n",
    "        return fig \n",
    "\n",
    "    # Récupérer le nœud survolé\n",
    "    try:\n",
    "        node_id = hoverData[\"points\"][0][\"customdata\"]\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        # Fallback en cas de structure inattendue\n",
    "        return dash.no_update\n",
    "\n",
    "    # Arêtes liées à ce nœud\n",
    "    filtered_edges = [e for e in G.edges() if node_id in e]\n",
    "\n",
    "    # Générer uniquement les segments des arêtes concernées\n",
    "    edge_x, edge_y = [], []\n",
    "    for edge in filtered_edges:\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x += [x0, x1, None]\n",
    "        edge_y += [y0, y1, None]\n",
    "\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        line=dict(width=2, color=\"#888\"),\n",
    "        hoverinfo=\"none\",\n",
    "        mode=\"lines\",\n",
    "    )\n",
    "    if node_size[node_text.index(node_id)] >= 20:\n",
    "        col = color_map[country] if country in color_map else node_color[node_text.index(node_id)]\n",
    "    else:\n",
    "        col = \"grey\"\n",
    "\n",
    "    color_trace = go.Scatter(\n",
    "        x=[pos[node_id][0]],\n",
    "        y=[pos[node_id][1]],\n",
    "        mode=\"markers + text\",\n",
    "        text=node_text[node_text.index(node_id)],\n",
    "        hoverinfo=\"text\",\n",
    "        marker=dict(\n",
    "            color=col,\n",
    "            size=node_size[node_text.index(node_id)],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(edge_trace)\n",
    "    fig.add_trace(color_trace)\n",
    "\n",
    "    return fig \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
